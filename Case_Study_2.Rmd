---
title: "Case Study 2"
author: "Michael Daniel Bigler and Liam Arthur Phan"
date: "`r Sys.Date()`"
output:
  rmdformats::downcute:
    code_folding: hide
    number_sections: TRUE
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(
	message = FALSE,
	warning = FALSE,
	include = TRUE,
	fig.align = "center")
rm(list = ls())
cat("\014")

```

<break>

> Goal: The goal of the research project discussed in the text is to develop a framework to measure product quality of smartphones as experienced by customers in terms of its constituent dimensions and to understand which quality dimensions significantly explain key outcome variables such as willingness to pay premium and repurchase intentions. We also will compare brand competitors in such factors and try to evaluate them with a common score scale. Thus, The goal of the project is to gain a deeper understanding of which quality dimensions significantly explain key outcome variables such as **willingness to pay premium** (**WTPP**) and **repurchase intentions** (**RI**) from customers. This information will be used by product management to improve the quality of their smartphones and enhance the company's competitive advantage in the marketplace.

# Packages

> In the following all the packages needed for this case study are loaded.

```{r packages}

library(ggplot2)
library(DT)
library(jtools)
library(kableExtra)
library(pastecs)
library(rcompanion)   #Histogram and Normal Curve
library(nortest) #Kolmogorov-Smirnov-Test
library(corrplot) #correlation matrix plot
library(olsrr)  #VIF and Tolerance Values
library(dplyr)
library(pastecs)
library(REdaS) #Bartelett's Test
library(psych)
library(lm.beta)
library(RColorBrewer)
library(tidyverse)
library(fastDummies)

```

# Data

> . The source is thereby listed below.

> Source: **Advanced Data Driven Decision Making** [S401024](https://moodle.unige.ch/course/view.php?id=2675 "2675") - University of Geneva (GSEM)
>
> Marcel Paulssen, Professor\
> Fereshteh Vahidi, Teaching Assistant\
> Anastasia Floru, Teaching Assistant

```{r data loading}
Mydata <- read.csv("DATA/Data File_Case_Study_Factor Analysis_MD.csv", header=TRUE)

# selecting questions
Mydata_with_y <- Mydata[,c(5, 9:41, 43:45, 51:52)]
Mydata_with_y <- na.omit(Mydata_with_y)
brands <- Mydata_with_y$brand
brands <- ifelse(brands == 1, "Apple", 
              ifelse(brands == 2, "Samsung",
              ifelse(brands == 3, "LG",
              ifelse(brands == 4, "Motorola", "Other"))))
Mydata_with_y <- Mydata_with_y[,c(2:39)]
Mydata_qd <- Mydata_with_y[,1:33]
rm(Mydata)
```

## Data Analysis

```{r, include=FALSE}
list_na =colnames(Mydata_with_y)[apply(Mydata_with_y,2,anyNA)]
list_na
```

## Data Statistics Summary

```{r}

Stat_Desc <- stat.desc(Mydata_with_y, norm=TRUE)

DT::datatable(round(Stat_Desc,3))

```


There is no missing values, kurtosis and skewness are relatively smalls, but kurtosis shows slight flat distributions. The skewness being often negative shows right skewed distribution of the data. 

> Comments: We can see that we have 969 observations for 33 questions (covariates), meaning we are above the required rule of thumb by **Frank Harrell's book, Regression Modeling Strategies** which states that for each covariate, 10-20 observations are needed. We would need 33x10 = 330 observations or 33x20 = 660 observations, and have way above with 969 observations [330 or 660 required]. None of the covariates are behaving normally.  

## **Histograms**

We can see the normality not full-filled visually as well. 

```{r}

plotNormalHistogram(Mydata_with_y$qd1,main = paste("Frequency Distribution of qd1"))

```
## **Normality Test on Y**

```{r}
lillie.test(Mydata_with_y$wtp1) 
lillie.test(Mydata_with_y$wtp2) 
lillie.test(Mydata_with_y$wtp3) 
lillie.test(Mydata_with_y$ri1) 
lillie.test(Mydata_with_y$ri2) 
```

As we can see, none of the response variables behave normally. 

## Correlation Plot

```{r}

cor_mat <- cor(Mydata_with_y)
corrplot(as.matrix(cor_mat), type="upper", order="hclust", tl.col="black", tl.srt=45,tl.cex = 0.5)

```

We can see that some questions have high correlations between them, but most of them being correlated positively between 0.4 and 0.6. 

# Multiple Linear Regression on All Covariates

## Compute mean scores for the three willingness to pay premium items (wtp1-wtp3) and the two repurchase intention items (ri1-ri2)

```{r}
Mydata_with_y$avg_wtp <- apply(Mydata_with_y[,34:36], 1, 'mean')
Mydata_with_y$avg_ri <- apply(Mydata_with_y[,37:38], 1, 'mean')
```

## Tolerance Value and VIF

### Willingness To Pay 

```{r}
model_wtp <- lm(avg_wtp ~., data = Mydata_with_y[c(1:33,39)])
ols_vif_tol(model_wtp)
```

The VIF are under control, with no values higher than 10. 

### Repurchase Intention

```{r}
model_ri <- lm(avg_ri ~., data = Mydata_with_y[c(1:33,40)])
ols_vif_tol(model_ri)
```


The VIF are under control, with no values higher than 10. 

Thus we shouldn't worry to much of the multicolinearity problem.

## Residuals

### Willingness to Pay

```{r}

par(mfrow = c(2, 2)) 
plot(model_wtp)

```

As we can see, the normality and homoskedasticity of the linear regression are not full-filled, seeing some patterns in the scale-location plot as well as out of place residuals in the Normal Q-Q on both sides of the plot. 

### Repurchase Intention

```{r}

par(mfrow = c(2, 2)) 
plot(model_ri)

```

As we can see, the normality and homoskedasticity of the linear regression are not full-filled, seeing some strong patterns in the scale-location plot as well as out of place residuals in the Normal Q-Q on both sides of the plot. This model for **Repurchase Intention** is way worse than the **Willingness to Pay**.

## Multiple Linear Regressoin

### Willingness to Pay

```{r}
model_wtp <- lm(avg_wtp ~., data = Mydata_with_y[c(1:33,39)])
model_wtp_beta=lm.beta(model_wtp)
summary(model_wtp_beta)
```
The adjusted R-squared is very low, thus only 39.7% of the variance on our depend variable is explained by the covariates, but the F-statistic is significant, showing that the covariates are indeed explaining something in the relation to **Willingness to Pay**. Variables qd23, qd5 and qd1 are significants at 1%, and qd23 being the most strongest predictive one.

### Repurchase Intention

```{r}
model_ri <- lm(avg_ri ~., data = Mydata_with_y[c(1:33,40)])
model_ri_beta=lm.beta(model_ri)
summary(model_ri_beta)
```

The adjusted R-squared is very low, thus only 45% of the variance on our depend variable is explained by the covariates, but the F-statistic is significant, showing that the covariates are indeed explaining something in the relation to **Repurchase Intention**. Variables qd30, qd23, qd18 and qd5 are significant (at different alpha level: 1% and 5%), and qd5 being the most strongest predictive one.

# Orthogonal Factor Analysis

## Anti-image correlation, Kaiser-Meyer-Olkin and Bartlett's test

### Anti-image Correlation

```{r}
KMOTEST=KMOS(Mydata_qd)
sort(KMOTEST$MSA)
```

All values are above 0.6 so a good factor analysis can be done. Only one value is close to not being good enough which is qd4. All the others are over 0.9.

### Bartlett's test

```{r}
bart_spher(Mydata_qd)
```

There is a correlation between variables, and the assumptions are not met since we have above 5 variables 

### Kaiser-Meyer-Olkin

```{r}
KMOTEST=KMOS(Mydata_qd)
KMOTEST$KMO
```

The Kaiser-Meyer-Olkin criterion suggest that the adequacy of the data is marvelous (>0.9). 

## Principal axes factoring with varimax

As we know from literature that there are 9 factors we do a factor analysis with 9. 

```{r}
fa_result <- fa(Mydata_qd, rotate = "varimax", fm = "pa")

plot(fa_result$e.values,xlab="Factor Number",ylab="Eigenvalue",main="Scree plot",cex.lab=1.2,cex.axis=1.2,cex.main=1.8)+abline(h=1)

factors_kaiser <- sum(fa_result$e.values>1)
```

Kaiser criterion gives `r factors_kaiser` factors, scree-test gives 10. From literature we know that there should be 9 factors. We first do a factor analysis with 8 factors to see if it makes sense. 

```{r}
fa_result_8 <- fa(Mydata_qd, rotate = "varimax", fm = "pa", nfactors = 8)
print(fa_result_8$loadings, cutoff=0.3,sort=TRUE)
```

We see that every question is loaded in a factor except for the question 4 and 23. We get similar results when we look at the communalities. 

```{r}
FA_communalities_8=data.frame(sort(fa_result_8$communality))
problematic_communalities_8 <- rownames(FA_communalities_8)[FA_communalities_8<0.5]
problematic_communalities_8
```

We see that there are two communalities which are below 0.5. They are fittingly for qd4 and qd23. 

As by literature there are 9 factors we rerun the factor analysis to see the loadings for 9 factors. 

```{r}
fa_result_9 <- fa(Mydata_qd, rotate = "varimax", fm = "pa", nfactors = 9)
print(fa_result_9$loadings, cutoff=0.3,sort=TRUE)
```

We can observe that the loadings for factor 9 are solely based on question 4 and question 23. We rerun the communalities

```{r}
FA_communalities_9=data.frame(sort(fa_result_9$communality))
problematic_communalities_9 <- rownames(FA_communalities_9)[FA_communalities_9<0.5]
problematic_communalities_9
```

Again we see that thetige.  question 4 and 23 are problematic. To see if we should exclude them we look at the questionnaire. Both question related to prestige therefore we leave them in and factor 9 is prestige. 

## Final Factor Solutions

```{r}

Aesthetics_Appearance <- Mydata_with_y[c("qd1","qd10","qd20","qd27")]
Durability <- Mydata_with_y[c("qd26","qd28","qd31")]
Ease_of_Use <- Mydata_with_y[c("qd3","qd11","qd13","qd30")]
Features_Versatility <- Mydata_with_y[c("qd6","qd8","qd18","qd25")]
Conformance <- Mydata_with_y[c("qd15","qd17","qd32")]
Performance <- Mydata_with_y[c("qd2","qd5","qd7","qd12","qd16")]
Reliability_Flawlessness <- Mydata_with_y[c("qd22","qd29","qd33")]
Serviceability <- Mydata_with_y[c("qd9","qd14","qd19","qd21","qd24")]
Distinctiveness_Prestige <- Mydata_with_y[c("qd4","qd23")]

All_Factors <- cbind(Aesthetics_Appearance,Durability,Ease_of_Use,Features_Versatility,Conformance,Performance,Reliability_Flawlessness,Serviceability,Distinctiveness_Prestige)

DT::datatable(Aesthetics_Appearance, caption = "Aesthetics & Appearance")
DT::datatable(Durability,caption = "Durability" )
DT::datatable(Ease_of_Use,caption = "Ease of Use")
DT::datatable(Features_Versatility,caption = "Features & Versatility")
DT::datatable(Conformance,caption = "Conformance")
DT::datatable(Performance,caption = "Performance")
DT::datatable(Reliability_Flawlessness,caption = "Reliability & Flawlessness")
DT::datatable(Serviceability,caption = "Serviceability")
DT::datatable(Distinctiveness_Prestige,caption = "Distinctiveness & Prestige")

DT::datatable(All_Factors, caption = "All Factors")

```

## Do all variables show clear loading patterns?

Yes they do. Factor 8 is the only one which has a loading on an other factor as well that is higher than 0.3. 

## Principal component analysis 

Here we run a principal component analysis. 

```{r}
PC1 <- principal(Mydata_qd, rotate = "varimax", nfactors = 9)
print(PC1$loadings, cutoff=0.3,sort=TRUE)
```

Comparing the results we see that the pca and fa deliver similar results. Every question is assigned to the same factor. The only thing that varies are the loadings themself. 

```{r}
PC_communalities_9=data.frame(sort(PC1$communality))
problematic_communalities_9 <- rownames(PC_communalities_9)[PC_communalities_9<0.5]
problematic_communalities_9
```

It is also observable that now there are no communalities which are below 0.5. So it assures us that we made the right decision before to run with 9 factors instead of 8. 

## What do the eigenvalues of the factors/quality dimensions tell us about their relevance for repurchase behaviour?

[Explanation](https://www.theanalysisfactor.com/factor-analysis-1-introduction/#:~:text=The%20eigenvalue%20is%20a%20measure,than%20a%20single%20observed%20variable.)

eigenvalues of factors explain how much variance of the assigned questions they cover. 




BELOW IS WRONG OR NOT NEEDED

The eigenvalues of the questions tell us how much variance is explained by the variables. The bigger the eigenvalue, the bigger the explained variance. To showcase this we show the eigenvalues of the principal component analysis and how much variance is explained by them. 

```{r}
EigenValue=PC1$values
Variance=EigenValue/ncol(Mydata_qd)*100
SumVariance=cumsum(EigenValue/ncol(Mydata_qd))
Total_Variance_Explained=cbind(EigenValue=EigenValue[EigenValue>0],Variance=Variance[Variance>0],Total_Variance=SumVariance[Variance>0])
Total_Variance_Explained
```

# Oblique Factor analysis


Here chose best one from above pca or fa and then change rotation 

```{r}
promax <- fa(Mydata_qd, oblique.scores = TRUE, rotate = 'promax', nfactors = 9)
```

## How high are the factors correlated and what is the highest correlation between factors. What does this mean – please explain

Just between factors --> correlation on scores matrix not using the structure matrix. 

```{r}
structure <- promax$Structure # correlation
pattern <- promax$loadings # weights
```


## Compute factor scores for your factors and name and label them appropriately

```{r}
scores <- promax$scores
```


## Run a regression analysis with the factor scores of the quality dimensions as independent and both the mean score of willingness to pay premium and repurchase intention as dependent variables

```{r}
Mydata_lm <- data.frame(scores, Mydata_with_y[, 39:40])
mod2 <- lm(avg_wtp ~., Mydata_lm[,-11])
mod3 <- lm(avg_ri ~., Mydata_lm[,-10])

summary(mod2)
summary(mod3)
```

## Interpret your results from a managerial perspective

## Do the results of the regression analysis for repurchase intentions differ across brands e.g. Samsung versus Apple versus LG

```{r}
Mydata_lm_2 <- data.frame(Mydata_with_y$avg_ri, brands)
Mydata_lm_2 <- dummy_cols(Mydata_lm_2, select_columns = 'brands', remove_selected_columns = TRUE, remove_first_dummy = TRUE)

mod4 <- lm(Mydata_with_y.avg_ri ~., Mydata_lm_2[-10])

summary(mod4)
```

## What do the results of these regressions imply? Please compare the brands on the factor scores

Mydata_qd with brands and then compute mean per brand?

```{r}
Mydata_brands <- data.frame(scores, brands)
aggregate(Mydata_brands[,1:9], by = list(Mydata_brands$brands), mean)
```

## What are points of parity and points of difference for the different brands. 




